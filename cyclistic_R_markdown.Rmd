---
title: "Cyclistic Bike-Share Analysis:<br>Comparing usage between members and non-members"
subtitle: "Capstone Project for Google Data Analytics Certificate"
author: "Brian Cox"
date: "March 5, 2024"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: false
    code_folding: hide
---
===============================================================================
\

# Background

Cyclistic is a fictional bikeshare company in Chicago.  A membership is not required to use the service, but some users do have memberships.  The company is conducting a campaign to convert its non-member users into members.  This analysis provides insight for that campaign by comparing how non-members and members use the service.

# Documents

This document was written in R Markdown and is a report on:

- all of the data cleaning and transformation process
- a small representative sample of the analysis process

I used R (with R Studio) and SQL (with BigQuery) to clean, transform, and analyze the data. I used Tableau and Excel for additional analysis, and Tableau for the final data visualization.

The resulting files ([GitHub repo](https://github.com/briancox-c/Cyclistic) | [Dropbox parent folder](https://www.dropbox.com/scl/fo/n2wmk5q83nbp3snvseyi1/h?rlkey=k2lullylu2md2po138ivncdyg&dl=0)) include:\

- R markdown - [GitHub](https://github.com/briancox-c/cyclistic/blob/main/cyclistic_R_markdown.Rmd) | [Dropbox](https://www.dropbox.com/scl/fi/5a28k49a7zuqf1b4564is/cyclistic_R_markdown.Rmd?rlkey=wpn1n1us50g4qslij7ojmi2mn&dl=0)
- R script - [GitHub](https://github.com/briancox-c/Cyclistic/blob/main/cyclistic_R_script.R) | [Dropbox](https://www.dropbox.com/scl/fi/9cm0twd4wsicwi6md0ik6/cyclistic_R_script.R?rlkey=l7lj57iva9ser37jqsp2w4c3v&dl=0)
- SQL script - [GitHub](https://github.com/briancox-c/Cyclistic/blob/main/cyclistic_SQL.sql) | [Dropbox](https://www.dropbox.com/scl/fi/clmundrrvvx0hnklk6km9/cyclistic_SQL.sql?rlkey=lve8p0tk8eyab7gr9ifcrl26a&dl=0)
- Tableau story - [Tableau Public](https://public.tableau.com/views/CyclisticStory_17111282404990/Cyclistic?:language=en-US&:sid=&:display_count=n&:origin=viz_share_link)
\

# Data

The dataset consists of .csv files covering the 5.7 million Cyclistic trips taken in 2023. Each record is an observation of one trip.  The data are open and are available [here](https://www.dropbox.com/scl/fi/lkjpfymd0jfbx1a6ovl8n/monthly_data.zip?rlkey=lprave7iebdst4wvvetz43qa2&dl=0)

The dataset contains the following variables (see appendix for schema):

1.	Ride ID: primary key
2.	User membership status: member or non-member (“casual”)
3.	Type of bicycle used: classic, electric, or docked
4.	Beginning and ending date-times of the trip, with precision in seconds
5.	Beginning and ending coordinates (latitude and longitude) of the trip, with precision to five decimal places (roughly 4 feet)
6.	Starting and ending station ID’s and station names\
\

#### Limitations

The data does not contain user identification.  This is a significant shortcoming because we don't know which users took which trips.\
\

# Data cleaning and Transformation

---

## Prepare environment and data
\

##### _Load packages_

```{r set-environment, message = FALSE, warning = FALSE}

packages <- c("here", "tidyverse", "data.table", "kableExtra")

install.packages(setdiff(packages, rownames(installed.packages())))

library(here)

library(tidyverse)

library(data.table)

library(kableExtra)
```
\

##### _Set some defaults using functions and options_

```{r set-defaults}

options(scipen = 9)

cable <- function(x) {
        kbl(x, format.args = list(big.mark = ",")) %>%
        kable_styling("hover", full_width = FALSE, position = "left",
                      font_size = 12)
}

theme_set(theme_minimal())

theme_update(legend.position = "none", plot.margin = margin(20,5,20,5))

blues <- c(member = "lightblue", casual = "navy")

month_labels <- c("Feb", "Apr", "Jun", "Aug", "Oct", "Dec")

knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```
\

##### _Compile monthly files into one yearly table_

```{r compile-data}

setwd(here("data/monthly"))

year_data <-
        list.files(pattern = "*.csv") %>% 
        lapply(function(x) fread(x)) %>% 
        rbindlist()

setwd(here())
```
\

##### _Preview compiled data_

```{r preview-data}

str(year_data)

summary(year_data)
```
\

## Address invalid or messy data
\
A few things stand out in the summary:

- The names of the columns "started_at" and "ended_at" make it unclear whether they refer to time or geography
- There are NA's in the end coordinates
- There are zero values in the end coordinates\
\

##### _Rename ambiguously named datetime columns_

```{r rename-columns}

colnames(year_data)[3] <- "time_start"

colnames(year_data)[4] <- "time_end"
```
\

##### _Investigate NA's_

```{r quantify-na}

colSums(is.na(year_data)) %>%
        kbl(col.names = NULL, format.args = list(big.mark = ",")) %>%
        kable_styling("hover", full_width = FALSE, position = "left",
                      font_size = 12)
```

```{r evaluate-na-coords}

number_na_end_coords <-
        nrow(year_data[is.na(end_lng) | is.na(end_lat), ])

percent_na_end_coords <-
        number_na_end_coords / nrow(year_data) * 100

cat(c("Records with missing end coordinates are",
      round(percent_na_end_coords, 2),
      "% of the total records"))
```
\
I can get rid of the records with missing end coordinates because

- they are a very small percentage of the records, and
- they are not an important subset of the data\
\
   
##### _Remove NA's from dataset_

```{r remove-na-coords}

year_data <-
        year_data %>% 
        filter(!is.na(end_lat) & !is.na(end_lng))
```
\

##### _Investigate zero coordinates_

```{r quantify-zero-coords}

number_zero_end_coords <-
        nrow(year_data[end_lng == 0 | end_lat == 0, ])

cat(c("There are",
      number_zero_end_coords,
      "records with zero for an ending longitude or latitude"))

year_data[end_lng == 0 | end_lat == 0, ] %>% cable()
```
\

There are 3 records with zero for an ending longitude or latitude. All 3 have the coordinates (0,0) which is in the ocean off of Ghana.\

I can remove those records but I notice that two of them are labeled in the station names or id's as "Test."  First I'll look for other records with "Test" labels\
\

##### _Investigate stations labeled "test"_

```{r identify-test-stations}

tests <-
        year_data[grepl("test", year_data$start_station_name,
                        ignore.case = TRUE) |
                  grepl("test", year_data$end_station_name,
                        ignore.case = TRUE) |
                  grepl("test", year_data$start_station_id,
                        ignore.case = TRUE) |
                  grepl("test", year_data$end_station_id,
                        ignore.case = TRUE), ]

nrow(tests)

view(tests)
```
\
Those are identified as tests and there are only 106 additional, so I'll remove them too.\
\

##### _Remove zero coordinates and tests from dataset_

```{r remove-zero-coords-and-tests}

year_data <-
        year_data %>% 
        filter(end_lat != 0 & end_lng !=0,
               !grepl("test", start_station_name, ignore.case = TRUE),
               !grepl("test", end_station_name, ignore.case = TRUE),
               !grepl("test", start_station_id, ignore.case = TRUE),
               !grepl("test", end_station_id, ignore.case = TRUE))
```
\

##### _Verify uniqueness of primary key values_

```{r verify-unique-ride-ids}

length(unique(year_data$ride_id)) == nrow(year_data)
```
\

##### _Look for invalid values in character columns with low cardinality_

```{r validate-character-columns}

unique(year_data$rideable_type)

unique(year_data$member_casual)
```
\

##### _Identify empty strings in each character column_

```{r quantify-empty-strings}

year_data %>%
        select_if(is.character) %>%
        sapply(function(x) sum(x == "")) %>% 
        kbl(col.names = NULL, format.args = list(big.mark = ",")) %>%
        kable_styling("hover", full_width = FALSE, position = "left",
                      font_size = 12)
```
\

##### _Investigate empty strings in station id's and station names_

```{r quantify-empty-station-ids}

percent_empty_start_id <-
        round(sum(year_data$start_station_id == "") / nrow(year_data) * 100, 2)

percent_empty_end_id <- 
        round(sum(year_data$end_station_id == "") / nrow(year_data) * 100, 2)
        
cat(paste(percent_empty_start_id,
      "% of the start station ids are empty, and",
    percent_empty_end_id, 
      "% of the end station ids are empty"))
```
\
That's too many to throw away, and those fields are just labels anyway, so the fact that they're missing doesn't compromise the data.

Since the station ID's and names are just labels, I can replace the empties with new values without changing the data substantively\

- For empty station id's, create a new station id by concatenating their latitude & longitude
- For empty station names, create new station names which are equal to the station id's\
\

##### _Substitute in new values for empty station id's and names_

```{r replace-empty-station-labels}

year_data <-
        year_data %>% 
        mutate(
                start_station_id = case_when(
                        start_station_id == '' ~ paste(start_lat,start_lng),
                        .default = as.character(start_station_id)),
                end_station_id = case_when(
                        end_station_id == '' ~ paste(end_lat,end_lng),
                        .default = as.character(end_station_id))
        )

year_data <- 
        year_data %>% 
        mutate(
                start_station_name = case_when(
                        start_station_name == '' ~ start_station_id,
                        .default = as.character(start_station_name)),
                end_station_name = case_when(
                        end_station_name == '' ~ end_station_id,
                        .default = as.character(end_station_name))
        )
```
\

#### Investigate datetime columns
\
The summary above showed that the values in each datetime column (start and end) are valid on their own.\
But I created and investigated a duration column (in minutes) to check out the relationship between the start and end times.\
\

##### _Create duration column and look for outliers and strange values_

```{r create-duration-column}

year_data$duration_mins <-
        as.numeric(difftime(year_data$time_end,
                            year_data$time_start, units = "mins"))

summary(year_data$duration_mins)
```

```{r identify-strange-durations}

na_durations <- sum(is.na(year_data$duration_mins))

negative_durations <- sum(year_data$duration_mins < 0)

zero_durations <- sum(year_data$duration_mins == 0)

dayplus_durations <- sum(year_data$duration_mins >= 1440)

strange_duration_report <- data.table(
        na_durations, negative_durations,
        zero_durations, dayplus_durations
)

percent_strange_durations <-
        sum(na_durations, negative_durations, zero_durations,
        dayplus_durations) / nrow(year_data) * 100

strange_duration_report %>% cable()

cat("records with strange durations are",
    round(percent_strange_durations, 2), "% of the total records")
```
\
I can remove the records with strange duration values because

- they are a very small percentage of the records, and
- they are not an important subset of the data\
\

##### _Remove records with strange duration values_

```{r remove-strange-durations}

year_data <-
        year_data[year_data$duration_mins > 0 &
        year_data$duration_mins < 1440, ]
```
\

##### _Re-summarize duration column to scan for remaining anomalies_

```{r validate-duration-column}

na_durations <- sum(is.na(year_data$duration_mins))

negative_durations <- sum(year_data$duration_mins < 0)

zero_durations <- sum(year_data$duration_mins == 0)

dayplus_durations <- sum(year_data$duration_mins >= 1440)

min_duration <- min(year_data$duration_mins) %>% round(4)

avg_duration <- mean(year_data$duration_mins) %>% round(2)

median_duration <- median(year_data$duration_mins) %>% round(2)

percentile_99.9 <- quantile(year_data$duration_mins, probs = 0.999) %>% round(1)

max_duration <- max(year_data$duration_mins) %>% round(1)

clean_duration_report <-
        data.table(na_durations, negative_durations, zero_durations,
                   dayplus_durations, min_duration, median_duration,
                   avg_duration, percentile_99.9, max_duration)

clean_duration_report %>% cable()

```
\

##### _Plot duration distribution to check if it generally makes sense_

```{r plot-duration, warning=FALSE}

ggplot(year_data, aes(x = duration_mins)) +
        geom_histogram(binwidth = 15) +
        xlim(0, 1500)

ggplot(year_data, aes(x = duration_mins, fill = member_casual)) +
        geom_histogram(binwidth = 5) +
        xlim(0, 100) +
        scale_fill_manual(values = blues) +
        facet_wrap(member_casual ~ .)
```
\
\

## Sample the data

The data is too big for the limited amount of RAM my laptop has.

Taking a 10% random sample will provide

- 99.99% confidence level, with
- less than 0.25% margin of error

Set seed first to ensure reproducibility\

**This sample will be the dataset used for the rest of the analysis**

```{r sample-data}

set.seed(0)

smp <- slice_sample(year_data, prop = 0.1)
```
\

## Export data and clean up environment
\

##### _Export .csv files for use in other tools_

eval is set to FALSE so no data is actually written to disk when knitting for review.

To keep the dataset tidy, the duration column is excluded from the export because it is a calculated field.

```{r export-sampled-table, eval = FALSE}

fwrite(smp[!"duration_mins"],
       here("data/yearly/sampled_yearly_data.csv"))

fwrite(year_data[!"duration_mins"],
       here("data/yearly/clean_whole_year.csv"))
```
\

##### _Clear unneeded objects from environment_

```{r clear-environment}

rm(
        tests,
        number_na_end_coords, percent_na_end_coords, number_zero_end_coords,
        na_durations, negative_durations, zero_durations, dayplus_durations,
        strange_duration_report, percent_strange_durations, min_duration,
        avg_duration, median_duration, percentile_99.9, max_duration,
        clean_duration_report, percent_empty_start_id, percent_empty_end_id,
        year_data
)
```
\

## Add calculated fields for use in analysis
\

Day Type

- Weekend: Saturday and Sunday
- Shoulder Weekday: Monday and Friday
- Middle Weekday: Tuesday, Wednesday, and Thursday

Trip Type

- Round Trip: start and end coordinates are the same
- One Way: start and end coordinates are different

Hour

- hour of the day the ride started

Month

- month of the year the ride started

```{r add-calculated-fields}

smp <- smp %>% 
        mutate(day_type =
                       case_when(wday(time_start) %in% c(1, 7) ~ "Weekend",
                                 wday(time_start) %in% c(2, 6) ~
                                         "Shoulder Weekday",
                                 wday(time_start) %in% 3:5 ~ "Middle Weekday"),
               trip_type =
                       case_when(paste(start_lat, start_lng) ==
                                         paste(end_lat, end_lng) ~ "Round Trip",
                                 .default = "One Way"),
               hour_start = hour(time_start),
               month_start = month(time_start)
        )
```
\

##### _Check newly created variables for validity and reasonableness_

```{r validate-calculated-fields}

table(smp$member_casual, smp$day_type) %>% cable()

table(smp$member_casual, smp$trip_type) %>% cable()

table(smp$member_casual, smp$hour_start) %>% cable()

table(smp$member_casual, smp$month_start) %>% cable()
```
\

#### Add geographically calculated fields
\

I used BigQuery to calculate the distance and direction of each ride because the same computations in R were too memory-intensive for my laptop. Below is the query I used.  I then exported the results from BigQuery.

- Distance: in miles between start and end coordinates
- Direction: cardinal direction from start to end, in integer degrees
\
\

##### _Calculate distance and direction_

```{sql, eval = FALSE, engine = "sql"}

DROP TABLE IF EXISTS cyclistic.geo;
CREATE TABLE cyclistic.geo AS
    SELECT
        ride_id,
        (ST_DISTANCE(ST_GEOGPOINT(end_lng, end_lat),
                ST_GEOGPOINT(start_lng, start_lat)))
                * 0.0006213712 ## convert meters to miles
                AS distance_miles,
        CAST(ST_AZIMUTH(ST_GEOGPOINT(start_lng, start_lat),
                ST_GEOGPOINT(end_lng, end_lat))
                * 57.29578 AS INT64) ## convert radians to degrees
                AS direction
    FROM cyclistic.smp
    WHERE trip_type = 'One Way';
```
\

##### _Read in geographically calculated data_

```{r read-geo-data}

geo <- fread(here("data/yearly/geo_calc_vars.csv"))
```
\
\

# Exploratory analysis

---

For brevity I have only included selected examples of the exploratory analysis in this file.

Because I did the final presentation in Tableau, I've given the graphics below enough formatting to be clear, but not as much as for a final presentation.

## Time clustering

```{r time-clustering}

ggplot(smp, aes(day_type, after_stat(prop), group = member_casual,
                fill = member_casual,
                label = paste(round(after_stat(prop)*100),"%"))) +
        geom_bar() +
        labs(title = "Percent of Rides by Day Type",
             subtitle = "Members rode more midweek, non-members weekends") +
        geom_text(stat = "count", color = "white", nudge_y = -0.05) +
        scale_y_continuous(label = scales::percent) +
        theme(axis.text.y=element_blank()) +
        scale_fill_manual(values = blues) +
        facet_grid(vars(member_casual))

ggplot(smp, aes(hour_start, after_stat(prop), fill = member_casual)) +
        geom_bar() +
        labs(title = "Percent of Rides by Hour and Day Type",
             subtitle = "Members had a bigger spike during the morning commute") +
        scale_y_continuous(label = scales::percent) +
        scale_fill_manual(values = blues) +
        facet_grid(vars(member_casual), vars(day_type))

ggplot(smp, aes(month_start, after_stat(prop), fill = member_casual)) +
        geom_bar() +
        labs(title = "Percent of Rides by Month and Day Type",
             subtitle = "Non-members' usage was more seasonal") +
        scale_x_continuous(breaks = c(2, 4, 6, 8, 10, 12),
                           labels = month_labels) +
        scale_y_continuous(label = scales::percent) +
        scale_fill_manual(values = blues) +
        facet_grid(vars(member_casual), vars(day_type))
```
\
Discussion:\
These differences, when combined with location data (maps and analysis are in the Tableau story) and the fact that non-members tend to take longer, slower rides (see below), suggest that non-members were more likely to use the service for recreation, while members were more likely to commute.  If the company could add or modify memberships to make them more attractive for recreational use, more non-members may convert.

However, the data also show that non-members did a lot of commuting as well as recreating.  Therefore the company should also seek conversions among commuting non-members.  Acquiring and analyzing ride data that includes unique user id (even if anonymized or de-identified) could help uncover reasons why non-member commuters haven't become members. For example, were they commuting too infrequently or irregularly to justify membership cost? 

## Direction

```{r direction}

dir_mwd_morning <- smp %>%
        right_join(geo, by = join_by(ride_id)) %>% 
        filter(day_type == 'Middle Weekday',
               hour_start %in% 5:8,
               distance_miles > 1)

dir_mwd_evening <- smp %>%
        right_join(geo, by = join_by(ride_id)) %>% 
        filter(day_type == 'Middle Weekday',
               hour_start %in% 15:18,
               distance_miles > 1)

ggplot(dir_mwd_morning, aes(direction, after_stat(prop), group = member_casual,
                            fill = member_casual)) +
        geom_bar() +
        labs(title = "Direction of Rides During Midweek Morning Commute") +
        scale_x_continuous(breaks = c(0, 90, 180, 270, 360)) +
        scale_y_continuous(label = scales::percent) +
        scale_fill_manual(values = blues) +
        facet_grid(vars(member_casual))

ggplot(dir_mwd_evening, aes(direction, after_stat(prop), group = member_casual,
                            fill = member_casual)) +
        geom_bar() +
        labs(title = "Direction of Rides During Midweek Evening Commute") +
        scale_x_continuous(breaks = c(0, 90, 180, 270, 360)) +
        scale_y_continuous(label = scales::percent) +
        scale_fill_manual(values = blues) +
        facet_grid(vars(member_casual))
```
\
Discussion:\
The overall NNW - SSE pattern is interesting, if unsurprising given the orientation of the the Chicago lakefront. The prevalence of SSE rides in the morning and NNW rides in the evening is also interesting. But there is not a meaningful difference between members and non-members, so this measure did not make it to the final analysis.\ 
\

## Distance, Duration, and Speed

```{r distance-duration-speed, warning = FALSE, message = FALSE}

smp %>% right_join(geo, by = join_by(ride_id)) %>%
        filter(rideable_type != 'docked_bike') %>%
        group_by(member_casual) %>% 
        summarise(median_duration_mins = median(duration_mins) %>% round(1),
                  median_distance_mi = median(distance_miles) %>% round(2),
                  median_speed_mph = median(distance_miles /
                                     (duration_mins / 60)) %>% round(1)) %>% 
        cable()

smp %>% right_join(geo, by = join_by(ride_id)) %>%
        filter(rideable_type != 'docked_bike',
               day_type != 'Shoulder Weekday') %>%
        group_by(day_type, member_casual) %>% 
        summarise(median_duration_mins = median(duration_mins) %>% round(1),
                  median_distance_mi = median(distance_miles) %>% round(2),
                  median_speed_mph = median(distance_miles /
                                     (duration_mins / 60)) %>% round(1)) %>% 
        kbl() %>%
        kable_styling("striped", full_width = FALSE, position = "left",
                      font_size = 12)
```
\

##### Distance

```{r distance, warning = FALSE}

quantile(geo$distance_miles, 0.9) %>% round(2) %>%
        kbl(col.names = NULL) %>%
        kable_styling(full_width = FALSE, position = "left",
                      font_size = 12)

dist_box <- smp %>%
        select(ride_id, member_casual, day_type, rideable_type) %>%
        right_join(geo, by = join_by(ride_id)) %>%
        filter(distance_miles > 0 & distance_miles <= 2.87,
               day_type != 'Shoulder Weekday',
               rideable_type != 'docked_bike'
        )

ggplot(dist_box, aes(rideable_type, distance_miles)) +
        geom_boxplot() +
        labs(title = "Median Distance of Rides by Bike Type and Day Type",
             subtitle = "Members used electric bikes for longer distances, but non-members did not") +
        facet_grid(vars(member_casual), vars(day_type))
```
\
Discussion:\
This difference may show that when commuting, members were more strategic about using electric bikes for longer rides to save time, and regular bikes for shorter rides to save money.  If the company could educate commuting non-members about strategies like this, they may be more likely to convert.

On the other hand, some of this difference may be that because non-members were more likely to use the service for recreation, they may be have chosen regular bikes more because they weren't trying to save time. 
\
\

##### Duration

```{r duration}

quantile(smp$duration_mins, 0.9) %>% round(2) %>%
        kbl(col.names = NULL) %>%
        kable_styling(full_width = FALSE, position = "left",
                      font_size = 12)

dur_box <- smp %>% 
        filter(duration_mins > 0 & duration_mins <= 29.03,
               day_type != "Shoulder Weekday")

ggplot(dur_box, aes(member_casual, duration_mins)) +
        geom_boxplot() +
        labs(title = "Median Duration of Rides by Day Type",
             subtitle = "Non-members rode for longer times, especially on weekends") +
        facet_grid(vars(day_type))

ggplot(dur_box, aes(member_casual, duration_mins)) +
        geom_boxplot() +
        labs(title = "Median Duration of Rides by Day Type and Month",
             subtitle = "The biggest difference was in the summer") +
        scale_x_discrete(labels = c("c", "m")) +
        facet_grid(vars(day_type), vars(month_start))
```
\
\

# Appendix

---

### Source data schema

```{r schema-table, echo = FALSE, message = FALSE, warning = FALSE}

schema <- data.table(
        Field = c("ride_id", "rideable_type", "started_at", "ended_at",
                       "start_station_name", "start_station_id",
                       "end_station_name", "end_station_id",
                       "start_lat", "start_lng", "end_lat", "end_lng",
                       "member_casual"),
        Type = c("character", "character", "datetime", "datetime",
                      "character", "character", "character", "character",
                      "numeric", "numeric", "numeric", "numeric",
                      "character"),
        Notes = c("Primary Key", "", "","", "", "", "", "", "", "", "", "",
                  ""))

schema %>% kbl() %>%
        kable_styling("hover", position = "left", full_width = FALSE,
                      font_size = 13) %>%
        column_spec(1, width = "12em") %>% 
        column_spec(2, width = "8em")
```

        
### Links
\

##### Project

- [GitHub](https://github.com/briancox-c/Cyclistic) | Code\

- [Dropbox](https://www.dropbox.com/scl/fo/n2wmk5q83nbp3snvseyi1/h?rlkey=k2lullylu2md2po138ivncdyg&dl=0) | Code and data\

- [Tableau Public](https://public.tableau.com/views/CyclisticStory_17111282404990/Cyclistic?:language=en-US&:sid=&:display_count=n&:origin=viz_share_link) | Tableau story\
\

##### Personal

- [LinkedIn](https://www.linkedin.com/in/brian-cox-6905a6256/)\

- [Coursera](https://www.coursera.org/user/1fcea3b66e06e9b1d206bc66e9aa4039)\

- [GitHub](https://github.com/briancox-c)\

- [Tableau Public](https://public.tableau.com/app/profile/brian.cox3159/vizzes)\
